{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anni1808/MINed_Hackoholics/blob/main/QuantumML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUtHO1i2Tuan",
        "outputId": "1f4f35f8-6d30-4e16-e9ad-dcf2176eaa1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Collecting tomlkit (from pennylane)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.1)\n",
            "Collecting pennylane-lightning>=0.40 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2024.12.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, rustworkx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, autoray, nvidia-cusparse-cu12, nvidia-cudnn-cu12, diastatic-malt, nvidia-cusolver-cu12, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.0 diastatic-malt-2.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane torch numpy pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and align datasets (keeping same preprocessing code)\n",
        "train_df = pd.read_csv(\"final_merged_dataset.csv\")\n",
        "test_df = pd.read_csv(\"aligned_test.csv\")\n",
        "\n",
        "def align_features(train_df, test_df):\n",
        "    train_cols = set(train_df.columns) - {'Type'}\n",
        "    test_cols = set(test_df.columns) - {'Type'}\n",
        "    common_cols = list(train_cols.intersection(test_cols))\n",
        "    common_cols.sort()\n",
        "    if 'Type' in train_df.columns:\n",
        "        common_cols.append('Type')\n",
        "    return train_df[common_cols], test_df[common_cols]\n",
        "\n",
        "def preprocess_features(df):\n",
        "    df_processed = df.copy()\n",
        "    for column in df_processed.columns:\n",
        "        if column != 'Type':\n",
        "            if df_processed[column].dtype == 'object':\n",
        "                df_processed[column] = df_processed[column].fillna('missing')\n",
        "                df_processed[column] = pd.util.hash_array(df_processed[column].values)\n",
        "                min_val = df_processed[column].min()\n",
        "                max_val = df_processed[column].max()\n",
        "                if min_val != max_val:\n",
        "                    df_processed[column] = (df_processed[column] - min_val) / (max_val - min_val)\n",
        "                else:\n",
        "                    df_processed[column] = 0\n",
        "            else:\n",
        "                df_processed[column] = df_processed[column].fillna(df_processed[column].mean())\n",
        "    return df_processed\n",
        "\n",
        "print(\"Original feature counts:\")\n",
        "print(f\"Training features: {len(train_df.columns) - 1}\")\n",
        "print(f\"Test features: {len(test_df.columns) - 1}\")\n",
        "\n",
        "train_df, test_df = align_features(train_df, test_df)\n",
        "print(f\"Number of features after alignment: {len(train_df.columns) - 1}\")\n",
        "\n",
        "# Preprocess features\n",
        "train_df = preprocess_features(train_df)\n",
        "test_df = preprocess_features(test_df)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['Type'] = label_encoder.fit_transform(train_df['Type'])\n",
        "test_df['Type'] = label_encoder.transform(test_df['Type'])\n",
        "\n",
        "# Prepare features\n",
        "X_train = train_df.drop(columns=['Type']).values\n",
        "y_train = train_df['Type'].values\n",
        "X_test = test_df.drop(columns=['Type']).values\n",
        "y_test = test_df['Type'].values\n",
        "\n",
        "# Remove constant features\n",
        "feature_std = np.std(X_train, axis=0)\n",
        "non_constant_features = feature_std > 0\n",
        "X_train = X_train[:, non_constant_features]\n",
        "X_test = X_test[:, non_constant_features]\n",
        "print(f\"Features after removing constants: {X_train.shape[1]}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Quantum settings\n",
        "n_qubits = 4\n",
        "print(f\"Using {n_qubits} qubits\")\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs, weights):\n",
        "    \"\"\"Quantum circuit for a single input\"\"\"\n",
        "    # Encode the first n_qubits features\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(inputs[i], wires=i)\n",
        "\n",
        "    # Apply entangling layers\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "\n",
        "    # Return expectations\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "class QuantumLayer(nn.Module):\n",
        "    def __init__(self, n_qubits):\n",
        "        super().__init__()\n",
        "        weight_shapes = {\"weights\": (3, n_qubits, 3)}\n",
        "        self.ql = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = x[:, :n_qubits]  # Take first n_qubits features\n",
        "\n",
        "        # Process each input in the batch separately\n",
        "        q_out = torch.stack([self.ql(x[i]) for i in range(batch_size)])\n",
        "        return q_out\n",
        "\n",
        "class QuantumClassifier(nn.Module):\n",
        "    def __init__(self, n_qubits, n_classes):\n",
        "        super().__init__()\n",
        "        self.quantum_layer = QuantumLayer(n_qubits)\n",
        "        self.post_process = nn.Sequential(\n",
        "            nn.Linear(n_qubits, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        q_out = self.quantum_layer(x)\n",
        "        return self.post_process(q_out)\n",
        "\n",
        "# Initialize model\n",
        "n_classes = len(np.unique(y_train))\n",
        "model = QuantumClassifier(n_qubits, n_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    # Shuffle data\n",
        "    indices = torch.randperm(len(X_train))\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "        if n_batches % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {n_batches}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / n_batches\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch_predictions = []\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        batch_X = X_test[i:i+batch_size]\n",
        "        outputs = model(batch_X)\n",
        "        batch_predictions.append(outputs.argmax(dim=1))\n",
        "\n",
        "    y_pred = torch.cat(batch_predictions)\n",
        "    accuracy = (y_pred == y_test).sum().item() / len(y_test)\n",
        "    print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(y_test.numpy(), y_pred.numpy())\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JURTEq74cVnH",
        "outputId": "9aee52bc-cb0c-40b5-f671-116f5c87ecde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original feature counts:\n",
            "Training features: 357\n",
            "Test features: 356\n",
            "Number of features after alignment: 356\n",
            "Features after removing constants: 305\n",
            "Using 4 qubits\n",
            "Epoch 1, Batch 10, Loss: 1.9301\n",
            "Epoch 1, Batch 20, Loss: 1.8164\n",
            "Epoch 1, Batch 30, Loss: 1.8936\n",
            "Epoch 1/20, Average Loss: 1.8576\n",
            "Epoch 2, Batch 10, Loss: 1.8315\n",
            "Epoch 2, Batch 20, Loss: 1.7053\n",
            "Epoch 2, Batch 30, Loss: 1.7377\n",
            "Epoch 2/20, Average Loss: 1.7819\n",
            "Epoch 3, Batch 10, Loss: 1.6849\n",
            "Epoch 3, Batch 20, Loss: 1.7959\n",
            "Epoch 3, Batch 30, Loss: 1.6166\n",
            "Epoch 3/20, Average Loss: 1.7093\n",
            "Epoch 4, Batch 10, Loss: 1.7425\n",
            "Epoch 4, Batch 20, Loss: 1.7062\n",
            "Epoch 4, Batch 30, Loss: 1.7922\n",
            "Epoch 4/20, Average Loss: 1.6494\n",
            "Epoch 5, Batch 10, Loss: 1.5698\n",
            "Epoch 5, Batch 20, Loss: 1.5380\n",
            "Epoch 5, Batch 30, Loss: 1.4659\n",
            "Epoch 5/20, Average Loss: 1.5974\n",
            "Epoch 6, Batch 10, Loss: 1.5530\n",
            "Epoch 6, Batch 20, Loss: 1.7105\n",
            "Epoch 6, Batch 30, Loss: 1.5856\n",
            "Epoch 6/20, Average Loss: 1.5725\n",
            "Epoch 7, Batch 10, Loss: 1.5186\n",
            "Epoch 7, Batch 20, Loss: 1.8421\n",
            "Epoch 7, Batch 30, Loss: 1.6712\n",
            "Epoch 7/20, Average Loss: 1.5589\n",
            "Epoch 8, Batch 10, Loss: 1.6083\n",
            "Epoch 8, Batch 20, Loss: 1.6219\n",
            "Epoch 8, Batch 30, Loss: 1.8434\n",
            "Epoch 8/20, Average Loss: 1.5403\n",
            "Epoch 9, Batch 10, Loss: 1.3711\n",
            "Epoch 9, Batch 20, Loss: 1.3663\n",
            "Epoch 9, Batch 30, Loss: 1.4510\n",
            "Epoch 9/20, Average Loss: 1.5053\n",
            "Epoch 10, Batch 10, Loss: 1.3987\n",
            "Epoch 10, Batch 20, Loss: 1.3242\n",
            "Epoch 10, Batch 30, Loss: 1.2684\n",
            "Epoch 10/20, Average Loss: 1.4799\n",
            "Epoch 11, Batch 10, Loss: 1.5312\n",
            "Epoch 11, Batch 20, Loss: 1.4872\n",
            "Epoch 11, Batch 30, Loss: 1.7072\n",
            "Epoch 11/20, Average Loss: 1.4598\n",
            "Epoch 12, Batch 10, Loss: 1.4966\n",
            "Epoch 12, Batch 20, Loss: 1.4270\n",
            "Epoch 12, Batch 30, Loss: 1.5884\n",
            "Epoch 12/20, Average Loss: 1.4483\n",
            "Epoch 13, Batch 10, Loss: 1.5114\n",
            "Epoch 13, Batch 20, Loss: 1.3722\n",
            "Epoch 13, Batch 30, Loss: 1.4009\n",
            "Epoch 13/20, Average Loss: 1.4322\n",
            "Epoch 14, Batch 10, Loss: 1.4193\n",
            "Epoch 14, Batch 20, Loss: 1.3521\n",
            "Epoch 14, Batch 30, Loss: 1.3566\n",
            "Epoch 14/20, Average Loss: 1.4134\n",
            "Epoch 15, Batch 10, Loss: 1.8480\n",
            "Epoch 15, Batch 20, Loss: 1.3248\n",
            "Epoch 15, Batch 30, Loss: 1.2446\n",
            "Epoch 15/20, Average Loss: 1.3964\n",
            "Epoch 16, Batch 10, Loss: 1.6850\n",
            "Epoch 16, Batch 20, Loss: 1.3035\n",
            "Epoch 16, Batch 30, Loss: 1.3309\n",
            "Epoch 16/20, Average Loss: 1.3760\n",
            "Epoch 17, Batch 10, Loss: 1.2605\n",
            "Epoch 17, Batch 20, Loss: 1.4724\n",
            "Epoch 17, Batch 30, Loss: 1.2242\n",
            "Epoch 17/20, Average Loss: 1.3608\n",
            "Epoch 18, Batch 10, Loss: 1.6304\n",
            "Epoch 18, Batch 20, Loss: 1.5715\n",
            "Epoch 18, Batch 30, Loss: 1.3018\n",
            "Epoch 18/20, Average Loss: 1.3486\n",
            "Epoch 19, Batch 10, Loss: 1.2311\n",
            "Epoch 19, Batch 20, Loss: 1.5715\n",
            "Epoch 19, Batch 30, Loss: 1.3622\n",
            "Epoch 19/20, Average Loss: 1.3592\n",
            "Epoch 20, Batch 10, Loss: 1.4060\n",
            "Epoch 20, Batch 20, Loss: 1.2612\n",
            "Epoch 20, Batch 30, Loss: 1.3031\n",
            "Epoch 20/20, Average Loss: 1.3327\n",
            "\n",
            "Test Accuracy: 20.68%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[306 340 195  87  63 489]\n",
            " [  0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0]]\n"
          ]
        }
      ]
    }
  ]
}